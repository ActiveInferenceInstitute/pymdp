# -*- coding: utf-8 -*-
"""opti_model_v3_RJP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j-s5mT8vGxBYytUq9wQ6-yDVpJfeDXf6

# OPTI Generative Model

## Setup
"""

# %% Imports

import os
import sys
import pathlib

path = pathlib.Path(os.getcwd())
module_path = str(path.parent) + '/'
sys.path.append(module_path)

import numpy as np
import pymdp
import copy
from pymdp import utils
from pymdp.utils import obj_array
from pymdp.agent import Agent

# %%
# States
states = {'person_type': [
            'child', 
            'adult'],
          
          'action_states': [
            'rest_close',
            'rest_medium',
            'rest_far',
            'wave_smile_close',
            'wave_smile_medium',
            'wave_smile_far',              
            'spin_close',
            'spin_medium',
            'spin_far',
            'dance_close',
            'dance_medium',
            'dance_far']
          }

# Observations
observations = {'action_outcomes': states['action_states'],
                'movement': ['toward_slow', 'toward_fast', 'stay', 'away_slow', 'away_fast'],
                'distance_from_target': ['close', 'medium', 'far'],
                'facial_expression': ['happy', 'not_happy'],
                'height': ['short', 'tall']
                }

num_states = [
    len(states[key]) for key in states
]

num_observations = [
    len(observations[key]) for key in observations
]

num_factors = len(num_states)
num_modalities = len(num_observations)

A = utils.obj_array_zeros([ [no, num_states[0], num_states[1]] for no in num_observations])

# Action outcomes
for idx in range(num_states[1]):
    A[0][idx,:,idx] = np.ones(num_states[0])

# Movement      
for idx in [0,3,6,9]:
    A[1][:,:,idx] = [[0, 0], [0, 0], [0.2, 0.1], [0.4, 0.4], [0.4, 0.5]]

for idx in [1,2]:
    A[1][:,:,idx] = [[0.18, 0.17], [0.17, 0.18], [0.3, 0.3], [0.18, 0.17], [0.17, 0.18]]

for idx in [4,7,10]:
    A[1][:,:,idx] = [[0.05, 0.1], [0.05, 0.1], [0.8, 0.6], [0.05, 0.1], [0.05, 0.1]]
    
for idx in [5,8,11]:
    A[1][:,:,idx] = [[0.3, 0.2], [0.4, 0.35], [0.1, 0.15], [0.1, 0.15], [0.1, 0.15]]

# Distance from target
for idx in [0,3,6,9]:
    A[2][0,:,idx] = np.ones(num_states[0])

for idx in [1,4,7,10]:
    A[2][1,:,idx] = np.ones(num_states[0])
    
for idx in [2,5,8,11]:
    A[2][2,:,idx] = np.ones(num_states[0])

# Facial expression 
for idx in range(2):
    A[3][:,:,idx] = [[0.5, 0.5], [0.5, 0.5]]

for idx in range(2,11):
    A[3][:,:,idx] = [[0.8, 0.9], [0.2, 0.1]]

# Height
for idx in range(11):
    A[4][:,:,idx] = np.eye(len(A[4]))
    

"""## B matrix"""

# setting up empty (w zeros) B matrix

num_actions = [2, 4]
B = utils.obj_array_zeros( [ [ns, ns, num_actions[f]] for f, ns in enumerate(num_states)])

# Transition matrix for person_type
for idx in range(num_states[0]):
    B[0][idx,idx,:] = 1

# Transition matrices for actions and directions
# Rest at medium
B[1][1,:,0] = 1

# Wave_Smile at medium
B[1][4,:,1] = 1

# Spin at medium
B[1][7,:,2] = 1

# Dance at medium
B[1][10,:,3] = 1

C = utils.obj_array_zeros( [no for no in num_observations])

C[1] = np.array([0, 0, 10, -2, -3])
C[2] = np.array([-2, 0, -5])
C[3] = np.array([5, -10])

D = utils.obj_array_uniform(num_states)
D[1] = np.eye(num_states[1])[1]

num_controls = [2, 4]
policies = pymdp.control.construct_policies(num_states, num_controls, policy_len=1)
agent = Agent(A = A, B = B, C = C, policies = policies)

B_gp = copy.deepcopy(B)
A_gp = copy.deepcopy(A)
# %%

"""#Agent loop"""
T = 10 # number of timesteps in the simulation

# Initial observation: agent is 
observation = [0,0,0,0,0]
state = [0,0]

for t in range(T):
    
    print(f"\nTime {t}:")
    
    print(f"State: {[(list(states)[f], states[list(states)[f]][state[f]]) for f in range(len(states))]}")
    print(f"Observations: {[(list(observations)[g], observations[list(observations)[g]][observation[g]]) for g in range(len(observations))]}")
    print(observation)
    belief_state = agent.infer_states(observation)
    print(agent.qs)
    agent.infer_policies()
    action = agent.sample_action()
    print(action)
    # update environment

    if t > 0: # update previous_location with current current_location
        state[0] = state[1]
    for f, s in enumerate(state):
        state[f] = utils.sample(B_gp[f][:, s, int(action[f])])
    for g, _ in enumerate(observation):
        observation[g] = utils.sample(A_gp[g][:, state[0], state[1]])

   # print(f"Beliefs: {[(list(states)[f], belief_state[f].values.round(3).T) for f in range(len(states))]}")
    
#     print(f"Action: {[(list(actions)[a], actions[list(actions)[a]][int(action[a])]) for a in range(len(states))]}")
#%%
belief_state = agent.infer_states([0, 0, 0, 0, 0, 0])

agent.infer_policies()

B[1][:,0,0]

agent.A[1]

A


# %%
