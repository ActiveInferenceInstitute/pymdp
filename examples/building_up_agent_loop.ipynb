{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.tree_util as jtu\n",
    "from jax import random as jr\n",
    "from pymdp.jax.agent import Agent as AIFAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan(f, init, xs, length=None):\n",
    "  if xs is None:\n",
    "    xs = [None] * length\n",
    "  carry = init\n",
    "  ys = []\n",
    "  for x in xs:\n",
    "    carry, y = f(carry, x)\n",
    "    ys.append(y)\n",
    "  \n",
    "  return carry, jnp.stack(ys)\n",
    "\n",
    "def evolve_trials(agent, env, num_timesteps):\n",
    "\n",
    "    def step_fn(carry, xs):\n",
    "        actions = carry['actions']\n",
    "        outcomes = carry['outcomes']\n",
    "        beliefs = agent.infer_states(outcomes, actions, *carry['args'])\n",
    "        q_pi, _ = agent.infer_policies(beliefs)\n",
    "        actions_t = agent.sample_action(q_pi)\n",
    "\n",
    "        outcome_t = env.step(actions_t)\n",
    "        outcomes = jtu.tree_map(lambda prev_o, new_o: jnp.stack([prev_o, jnp.expand_dims(new_o, 0)], 0), outcomes, outcome_t)\n",
    "\n",
    "        actions = jnp.stack([actions, jnp.expand_dims(actions_t, 0)], 0) if actions is not None else actions_t\n",
    "        args = agent.update_empirical_prior(actions_t, beliefs)\n",
    "        # (pred, [cond_1, ..., cond_{t-1}])\n",
    "\n",
    "        # ovf beliefs = (post_T, [cond_1, cond_2, ..., cond_{T-1}])\n",
    "        # else beliefs = (post_T, post_{T-1}, ..., post_1)\n",
    "        return {'args': args, 'outcomes': outcomes, 'beliefs': beliefs, 'actions': actions}, None\n",
    "\n",
    "    outcome_0  = env.step()\n",
    "    init = ((agent.D, None), outcome_0, None, None)\n",
    "    last, _ = scan(step_fn, init, range(num_timesteps))\n",
    "\n",
    "    return last\n",
    "\n",
    "def step_fn(carry, b):\n",
    "    agent = carry\n",
    "    output = evolve_trials(agent, b)\n",
    "\n",
    "    # How to deal with contiguous blocks of trials? Two options we can imagine: \n",
    "    # A) you use final posterior (over current and past timesteps) to compute the smoothing distribution over qs_{t=0} and update pD, and then pass pD as the initial state prior ($D = \\mathbb{E}_{pD}[qs_{t=0}]$);\n",
    "    # B) we don't assume that blocks 'reset time', and are really just adjacent chunks of one long sequence, so you set the initial state prior to be the final output (`output['beliefs']`) passed through\n",
    "    # the transition model entailed by the action taken at the last timestep of the previous block.\n",
    "    \n",
    "    agent = agent.learning(**output)\n",
    "    \n",
    "    return agent, output\n",
    "\n",
    "init_agent = agent\n",
    "agent, squences = scan(step_fn, init_agent, range(num_blocks) )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
