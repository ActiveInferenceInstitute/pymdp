{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "\n",
    "from pymdp.core.control import get_expected_states\n",
    "from pymdp.core.inference import update_posterior_states\n",
    "from pymdp.utils import sample\n",
    "\n",
    "from pymdp.genmodels import TwoArmedBandit_gm\n",
    "\n",
    "A, B, C, D, policies = TwoArmedBandit_gm.get_model_parameters()\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define env here\n",
    "\n",
    "class Bernoulli_TwoArmedBandit(Env):\n",
    "\n",
    "    def __init__(self, reward_prob_arm_1 = 0.8, reward_prob_arm_2 = 0.2):\n",
    "        \n",
    "        self._reward_dist_arm_1 = np.array([reward_prob_arm_1, 1.0 - reward_prob_arm_1])\n",
    "        self._reward_dist_arm_2 = np.array([reward_prob_arm_2, 1.0 - reward_prob_arm_2])\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Given an action integer of which bandit-arm to play, samples the Bernoulli\n",
    "        distribution over rewards (0 = rewarding, 1 = not rewarding) for the selected\n",
    "        bandit arm\n",
    "        \"\"\"\n",
    "\n",
    "        if action == 0:\n",
    "            observation = utils.sample(self._reward_dist_arm_1)\n",
    "        elif action == 1:\n",
    "            observation = utils.sample(self._reward_dist_arm_2)\n",
    "        \n",
    "        return observation\n",
    "        \n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define bespoke Agent class here \n",
    "\n",
    "class Agent(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.A = A_matrix\n",
    "        self.B = B_matrix\n",
    "\n",
    "    def sample_action(obs):\n",
    "        \n",
    "        qs = self.infer_states(obs)\n",
    "\n",
    "        q_pi, efe = self.infer_policies()\n",
    "\n",
    "        action = self.sample_action()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def infer_states(self, observation):\n",
    "        observation = tuple(observation)\n",
    "\n",
    "        if not hasattr(self, \"qs\"):\n",
    "            self.reset()\n",
    "\n",
    "        if self.action is not None:\n",
    "            empirical_prior = control.get_expected_states(\n",
    "                self.qs, self.B, self.action.reshape(1, -1) \n",
    "            ).log() \n",
    "        else:\n",
    "            empirical_prior = self.D.log()\n",
    "\n",
    "        qs = update_posterior_states(self.A, observation, empirical_prior)\n",
    "       \n",
    "        self.qs = qs\n",
    "\n",
    "        return qs\n",
    "    \n",
    "    def infer_policies(self):\n",
    "\n",
    "        q_pi, efe = control.update_posterior_policies(\n",
    "                self.qs,\n",
    "                self.A,\n",
    "                self.B,\n",
    "                self.C,\n",
    "                self.policies,\n",
    "                self.use_utility,\n",
    "                self.use_states_info_gain,\n",
    "                self.use_param_info_gain,\n",
    "                self.pA,\n",
    "                self.pB,\n",
    "                self.gamma,\n",
    "                return_numpy=False,\n",
    "            )\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the agent-environment interaction loop using the OpenAIGym formalism\n",
    "\n",
    "agent = Agent()\n",
    "\n",
    "for t in range(T):\n",
    "\n",
    "    action = agent.get_action(observations)\n",
    "\n",
    "    observation, ..., info = env.step(action)\n",
    "\n",
    "    env.render() # this should wrap up print statements that provide timestep specific info (current state, etc.)\n",
    "\n"
   ]
  }
 ]
}