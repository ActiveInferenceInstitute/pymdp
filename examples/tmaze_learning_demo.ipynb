{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Demo: T-Maze Environment + Learning\n",
    "This notebook gives a step-by-step demonstration of active inference and learning using the `Agent()` class of `pymdp` and the `TMazeEnv` environment.\n",
    "\n",
    "For a thorough introduction to the basic `TMazeEnv` used here and active inference _without learning_ in that environment, please see the main `tmaze_demo.ipynb` notebook.\n",
    "\n",
    "In this particular case, we assume that the contingencies describing the 'rules' of the T-Maze are unknown to the agent, and must be learned. This demo therefore requires understanding the distinction between the agent's _generative model_  and the _generative process_ (the actual rules governing world dynamics).\n",
    "\n",
    "'Learning' under active inference corresponds to updating posterior beliefs about the sufficient statistics of distributions that parameterize the so-called 'structure' of the generative model. In the case of the Categorical distributions used to represent beliefs about states and outcomes in the discrete space-and-time MDPs used in `pymdp`, these structural parameters correspond to prior beliefs about the sensory and transition likelihoods and priors (the `A`, `B`, `C`, and `D` arrays) of the generative model. \n",
    "\n",
    "In the following exercise, we consider the case when an active inference agent needs to learn how sensory cues (the equivalent of a 'conditioned stimulus' or CS in learning theory) indicate the probability of receiving a reward in the two arms of a T-Maze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports|\n",
    "\n",
    "First, import `pymdp` and the modules we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "path = pathlib.Path(os.getcwd())\n",
    "module_path = str(path.parent) + '/'\n",
    "sys.path.append(module_path)\n",
    "\n",
    "from pymdp.agent import Agent\n",
    "from pymdp import utils\n",
    "from pymdp.envs import TMazeEnvNullOutcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Functions\n",
    "\n",
    "Define some utility functions that will be helpful for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_beliefs(belief_dist, title=\"\"):\n",
    "    plt.grid(zorder=0)\n",
    "    plt.bar(range(belief_dist.shape[0]), belief_dist, color='r', zorder=3)\n",
    "    plt.xticks(range(belief_dist.shape[0]))\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_likelihood(A, title=\"\"):\n",
    "    ax = sns.heatmap(A, cmap=\"OrRd\", linewidth=2.5)\n",
    "    plt.xticks(range(A.shape[1]))\n",
    "    plt.yticks(range(A.shape[0]))\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_probabilities = [0.85, 0.15] # the 'true' reward probabilities \n",
    "env = TMazeEnvNullOutcome(reward_probs = reward_probabilities)\n",
    "A_gp = env.get_likelihood_dist()\n",
    "B_gp = env.get_transition_dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pA = utils.dirichlet_like(A_gp, scale = 1e16)\n",
    "\n",
    "pA[1][1:,1:3,:] = 1.0\n",
    "\n",
    "A_gm = utils.norm_dist_obj_arr(pA)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_likelihood(A_gm[1][:,:,0],'Initial beliefs about reward mapping')\n",
    "plot_likelihood(A_gp[1][:,:,0],'True reward mapping in RIGHT ARM (Condition 0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_likelihood(A_gm[2][:,:,0],'Initial beliefs about cue mapping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_gm = copy.deepcopy(B_gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controllable_indices = [0] # this is a list of the indices of the hidden state factors that are controllable\n",
    "learnable_modalities = [1] # this is a list of the modalities that you want to be learn-able "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(A=A_gm,pA=pA,B=B_gm,\n",
    "              control_fac_idx=controllable_indices,\n",
    "              modalities_to_learn=learnable_modalities,\n",
    "              lr_pA = 0.25,\n",
    "              use_param_info_gain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.D[0] = utils.onehot(0, agent.num_states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.C[1][1] = 2.0\n",
    "agent.C[1][2] = -2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000 # number of timesteps\n",
    "\n",
    "obs = env.reset() # reset the environment and get an initial observation\n",
    "\n",
    "# these are useful for displaying read-outs during the loop over time\n",
    "reward_conditions = [\"Right Arm Better\", \"Left arm Better\"]\n",
    "location_observations = ['CENTER','RIGHT ARM','LEFT ARM','CUE LOCATION']\n",
    "reward_observations = ['No reward','Reward!','Loss!']\n",
    "cue_observations = ['Null','Cue Right','Cue Left']\n",
    "msg = \"\"\" === Starting experiment === \\n Reward condition: {}, Observation: [{}, {}, {}]\"\"\"\n",
    "print(msg.format(reward_conditions[env.reward_condition], location_observations[obs[0]], reward_observations[obs[1]], cue_observations[obs[2]]))\n",
    "\n",
    "pA_history = []\n",
    "\n",
    "all_actions = np.zeros((T, 2))\n",
    "for t in range(T):\n",
    "    \n",
    "    qx = agent.infer_states(obs)\n",
    "\n",
    "    q_pi, efe = agent.infer_policies()\n",
    "\n",
    "    action = agent.sample_action()\n",
    "\n",
    "    pA_t = agent.update_A(obs)\n",
    "    pA_history.append(pA_t)\n",
    "    \n",
    "    msg = \"\"\"[Step {}] Action: [Move to {}]\"\"\"\n",
    "    print(msg.format(t, location_observations[int(action[0])]))\n",
    "\n",
    "    obs = env.step(action)\n",
    "\n",
    "    all_actions[t,:] = action\n",
    "\n",
    "    msg = \"\"\"[Step {}] Observation: [{},  {}, {}]\"\"\"\n",
    "    print(msg.format(t, location_observations[int(obs[0])], reward_observations[int(obs[1])], cue_observations[int(obs[2])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_likelihood(agent.A[1][1:,:,0],'Final beliefs about reward contingencies under the condition that the reward condition is RIGHT ARM BETTER')\n",
    "plot_likelihood(A_gp[1][1:,:,0],'True reward contingencies under the condition that the reward condition is RIGHT ARM BETTER')\n",
    "\n",
    "plot_likelihood(agent.A[1][1:,:,1],'Final beliefs about reward contingencies under the condition that the reward condition is LEFT ARM BETTER')\n",
    "plot_likelihood(A_gp[1][1:,:,0],'True reward contingencies under the condition that the reward condition is LEFT ARM BETTER')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_likelihood(agent.A[2][1:,:,1],'Final beliefs about cue mapping')\n",
    "plot_likelihood(A_gp[2][1:,:,1],'True contingencies underlying the cue mapping')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "24ee14d9f6452059a99d44b6cbd71d1bb479b0539b0360a6a17428ecea9f0810"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('pymdp_env2': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
