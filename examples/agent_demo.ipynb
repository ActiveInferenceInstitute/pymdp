{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Inference Demo: T-Maze Environment\n",
    "This demo notebook provides a full walk-through of how to build a POMDP agent's generative model and perform active inference routine (inversion of the generative model) using the `Agent()` class of `pymdp`. We build a generative model from 'ground up', directly encoding our own A, B, and C matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "First, import `pymdp` and the modules we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = pathlib.Path(os.getcwd())\n",
    "module_path = str(path.parent) + '/'\n",
    "sys.path.append(module_path)\n",
    "\n",
    "from pymdp.agent import Agent\n",
    "from pymdp.core import utils\n",
    "from pymdp.core.maths import softmax\n",
    "from pymdp.distributions import Categorical, Dirichlet\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model dimensions and intuitive labels for the different model components\n",
    "\n",
    "Define the observation modalities, the hidden state factors, and the actions. We define them in terms of their dimensionalities and give each one an intuitive label that we can use later on to validate that our agent is acting in agreement with how we designed their generative model.\n",
    "\n",
    "We assume the agent's \"represents\" (read: generative model) its environment using two latent variables, that are factorized - in practice this means we can neatly segregate them out in the generative model as well as in the agent's variational posterior (that comes later).\n",
    "\n",
    "These two hidden state factors are `GAME STATE` and `PLAYING_VS_SAMPLING`. \n",
    "\n",
    "The first factor is a binary variable representing whether the state of the world is one that yields rewards with high probability (`GAME_STATE = 0`) or one that yields punishments (inverse rewards) with high probability (`GAME_STATE = 1`). You can think of this as basically the 'pay-off' structure of e.g. a slot machine or some game that the agent is currently playing. Crucially, the agent doesn't _know_ what the `GAME_STATE` is. They will have to infer it.\n",
    "\n",
    "The second factor is a ternary (3-valued) variable representing ....\n",
    "\n",
    "The observation modalities themselves are divided into 3 modalities:\n",
    "\n",
    "The first obs modality...\n",
    "\n",
    "the second obs modality ...\n",
    "\n",
    "the third obs modality ...\n",
    "\n",
    "\n",
    "N.B. Useful define the total number of modalities and factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_names = [\"GAME_STATE_OBS\", \"REWARD\", \"ACTION_SELF_OBS\"]\n",
    "state_names = [\"GAME_STATE\", \"PLAYING_VS_SAMPLING\"]\n",
    "action_names = [\"NULL\", \"PLAY_SAMPLE_INIT\"]\n",
    "\n",
    "num_obs = [3, 3, 3]\n",
    "num_states = [2, 3]\n",
    "num_modalities = len(num_obs)\n",
    "num_factors = len(num_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up observation likelihood matrix - first main component of generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = utils.obj_array_zeros([[o] + num_states for _, o in enumerate(num_obs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First modality : describe it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[0][:, :, 0] = np.ones( (num_obs[0], num_states[0]) ) / num_obs[0]\n",
    "A[0][:, :, 1] = np.ones( (num_obs[0], num_states[0]) ) / num_obs[0]\n",
    "A[0][:, :, 2] = np.array([[0.8, 0.2], [0.0, 0.0], [0.2, 0.8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second modality: describe it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[1][2, :, 0] = np.ones(num_states[0])\n",
    "A[1][0:2, :, 1] = softmax(np.eye(num_obs[1] - 1)) # bandit statistics (mapping between reward-state (first hidden state factor) and rewards (Good vs Bad))\n",
    "A[1][2, :, 2] = np.ones(num_states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third modality: describe it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[2][0,:,0] = 1.0\n",
    "A[2][1,:,1] = 1.0\n",
    "A[2][2,:,2] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Controllable-) Transition Dynamics\n",
    "\n",
    "Importantly, some hidden state factors are _controllable_ by the agent, meaning that the probability of being in state $i$ at $t+1$ isn't merely a function of the state at $t$, but also of actions (or from the agent's perspective, _control states_ ). So now each transition likelihood encodes conditional probability distributions over states at $t+1$, where the conditioning variables are both the states at $t-1$ _and_ the actions at $t-1$. This extra conditioning on actions is encoded via an optional third dimension to each factor-specific `B` matrix.\n",
    "\n",
    "For example, in our case the first hidden state factor (`Location`) is under the control of the agent, which means the corresponding transition likelihoods `B[0]` are index-able by both previous state and action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_fac_idx = [1]\n",
    "B = utils.obj_array(num_factors)\n",
    "for f, ns in enumerate(num_states):\n",
    "    B[f] = np.eye(ns)\n",
    "    if f in control_fac_idx:\n",
    "        # maybe do this bit more transparently to help with understand - i.e. loop over actions and fill out\n",
    "        # each action using B[f][action,:,action] = 1.0\n",
    "        B[f] = B[f].reshape(ns, ns, 1)\n",
    "        B[f] = np.tile(B[f], (1, 1, ns))\n",
    "        B[f] = B[f].transpose(1, 2, 0)\n",
    "    else:\n",
    "        B[f] = B[f].reshape(ns, ns, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C matrix / utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = utils.obj_array_zeros([num_ob for num_ob in num_obs])\n",
    "C[1][0] = 1.0  # put a 'reward' over first observation\n",
    "C[1][1] = -5.0  # put a 'punishment' over first observation\n",
    "# this implies that C[1][2] is 'neutral'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(A=A, B=B, C=C, control_fac_idx=control_fac_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up our simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial state\n",
    "T = 5\n",
    "o = [2, 2, 0]\n",
    "s = [0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative process - important note how the generative process doesn't have to be described by A and B matrices - can just be the arbitrary 'rules of the game' that you 'write in' as a modeller. But here we just use the same transition/likelihood matrices to make the sampling process straightforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition/observation matrices characterising the generative process\n",
    "A_gp = copy.deepcopy(A)\n",
    "B_gp = copy.deepcopy(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Observation GAME_STATE_OBS: 0\n",
      "0: Observation REWARD: 2\n",
      "0: Observation ACTION_SELF_OBS: 2\n",
      "0: Beliefs about GAME_STATE: [[1.]\n",
      " [0.]]\n",
      "0: Beliefs about PLAYING_VS_SAMPLING: [[0.]\n",
      " [0.]\n",
      " [1.]]\n",
      "0\n",
      "0: Action: [0 0] / State: [0, 0]\n",
      "1: Observation GAME_STATE_OBS: 1\n",
      "1: Observation REWARD: 2\n",
      "1: Observation ACTION_SELF_OBS: 0\n",
      "1: Beliefs about GAME_STATE: [[1.]\n",
      " [0.]]\n",
      "1: Beliefs about PLAYING_VS_SAMPLING: [[1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "0\n",
      "1: Action: [0 0] / State: [0, 0]\n",
      "2: Observation GAME_STATE_OBS: 0\n",
      "2: Observation REWARD: 2\n",
      "2: Observation ACTION_SELF_OBS: 0\n",
      "2: Beliefs about GAME_STATE: [[1.]\n",
      " [0.]]\n",
      "2: Beliefs about PLAYING_VS_SAMPLING: [[1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "0\n",
      "2: Action: [0 0] / State: [0, 0]\n",
      "3: Observation GAME_STATE_OBS: 2\n",
      "3: Observation REWARD: 2\n",
      "3: Observation ACTION_SELF_OBS: 0\n",
      "3: Beliefs about GAME_STATE: [[1.]\n",
      " [0.]]\n",
      "3: Beliefs about PLAYING_VS_SAMPLING: [[1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "0\n",
      "3: Action: [0 0] / State: [0, 0]\n",
      "4: Observation GAME_STATE_OBS: 2\n",
      "4: Observation REWARD: 2\n",
      "4: Observation ACTION_SELF_OBS: 0\n",
      "4: Beliefs about GAME_STATE: [[1.]\n",
      " [0.]]\n",
      "4: Beliefs about PLAYING_VS_SAMPLING: [[1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "0\n",
      "4: Action: [0 0] / State: [0, 0]\n"
     ]
    }
   ],
   "source": [
    "for t in range(T):\n",
    "\n",
    "    for g in range(num_modalities):\n",
    "        print(f\"{t}: Observation {obs_names[g]}: {o[g]}\")\n",
    "\n",
    "    qx = agent.infer_states(o)\n",
    "\n",
    "    for f in range(num_factors):\n",
    "        print(f\"{t}: Beliefs about {state_names[f]}: {qx[f].values.round(3)}\")\n",
    "\n",
    "    agent.infer_policies()\n",
    "    action = agent.sample_action()\n",
    "\n",
    "    for f, s_i in enumerate(s):\n",
    "        s[f] = utils.sample(B_gp[f][:, s_i, action[f]])\n",
    "\n",
    "    for g, _ in enumerate(o):\n",
    "        o[g] = utils.sample(A_gp[g][:, s[0], s[1]])\n",
    "    \n",
    "    print(np.argmax(s))\n",
    "    print(f\"{t}: Action: {action} / State: {s}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
