{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Inference model inversion: T-Maze Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from copy import deepcopy\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pymdp.jax.agent import Agent\n",
    "from pymdp.envs import TMazeEnv\n",
    "from pymdp import utils \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "Here we consider an agent navigating a three-armed 'T-maze,' with the agent starting in a central location of the maze. The bottom arm of the maze contains an informative cue, which signals in which of the two top arms ('Left' or 'Right', the ends of the 'T') a reward is likely to be found. \n",
    "\n",
    "### Hidden states\n",
    "\n",
    "The T-Maze environment is comprised of two hidden state factors:\n",
    "\n",
    "- `Location`: a $4$-dimensional vector that encodes the current position of the agent, and can take the following values: {`CENTER`, `RIGHT ARM`, `LEFT ARM`, or `CUE LOCATION`}. For example, if the agent is in the `CUE LOCATION`, the current state of this factor would be $s_1 = [0 \\ 0 \\ 0 \\ 1]$.\n",
    "\n",
    "-`Reward Condition`: a $ 1 \\ x \\ 2 $ vector that encodes the reward condition of the trial: {`Reward on Right`, or `Reward on Left`}.  A trial where the condition is reward is `Reward on Left` is thus encoded as the state $s_2 = [0 \\ 1]$.\n",
    "\n",
    "The environment is designed such that when the agent is located in the `RIGHT ARM` and the reward condition is `Reward on Right`, the agent has a specified probability $a$ (where $a > 0.5$) of receiving a reward, and a low probability $b = 1 - a$ of receiving a 'loss' (we can think of this as an aversive or unpreferred stimulus). If the agent is in the `LEFT ARM` for the same reward condition, the reward probabilities are swapped, and the agent experiences loss with probability $a$, and reward with lower probability $b = 1 - a$. These reward contingencies are intuitively swapped for the `Reward on Left` condition. \n",
    "\n",
    "### Observations\n",
    "\n",
    "The agent is equipped with three sensory channels or observation modalities: `Location`, `Reward`, and `Cue`. \n",
    "\n",
    "-  `Location`: a $4$-dimensional observation that encodes the sensed position of the agent, and can take the same values as the `Location` hidden state factor.\n",
    "  \n",
    "- `Reward` : a $3$-dimensional observation that can take the values `No Reward`, `Reward` or `Loss`.  The `No Reward` (index 0) observation is  observed whenever the agent isn't occupying one of the two T-maze arms (the right or left arms). The `Reward` (index 1) and `Loss` (index 2) observations are observed in the right and left arms of the T-maze, with associated probabilities that depend on the reward condition (i.e. on the value of the second hidden state factor).\n",
    "\n",
    "- `Cue`: a $2$-dimensional observation that can take the values `Cue Right` or `Cue Left`. This observation signals the reward condition of the trial, and therefore in which arm the `Reward` observation is more probable. When the agent occupies the other two arms (the `RIGHT` or `LEFT` arms), the `Cue` observation will be `Cue Right` or `Cue Left` with equal probability. However (as we'll see below when we intialise the agent), the agent's beliefs about the likelihood mapping render these observations uninformative and irrelevant to state inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize environment\n",
    "Now we can initialize the T-maze environment using the built-in `TMazeEnv` class from the `pymdp.envs` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose reward probabilities $a$ and $b$, where $a$ and $b$ are the probabilities of reward / loss in the 'correct' arm, and the probabilities of loss / reward in the 'incorrect' arm. Which arm counts as 'correct' vs. 'incorrect' depends on the reward condition (state of the 2nd hidden state factor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_probabilities = [0.98, 0.02] # probabilities used in the original SPM T-maze demo\n",
    "env = TMazeEnv(reward_probs = reward_probabilities)\n",
    "A_gp = env.get_likelihood_dist()\n",
    "B_gp = env.get_transition_dist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on Controllable (and Uncontrollable-) Transition Dynamics\n",
    "\n",
    "Importantly, some hidden state factors are _controllable_ by the agent, meaning that the probability of being in state $i$ at $t+1$ doesn't only depend on the state at $t$, but also on actions or _control states_. So now each transition likelihood encodes conditional probability distributions over states at $t+1$, where the conditioning variables are both the states at $t-1$ _and_ the actions at $t-1$. This extra conditioning on actions is encoded via an optional third dimension to each factor-specific `B` matrix.\n",
    "\n",
    "For example, in our case the first hidden state factor (`Location`) is under the control of the agent, which means the corresponding transition likelihoods `B[0]` are index-able by both previous state and action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The generative model\n",
    "Now we can move onto setting up the generative model of the agent - namely, the agent's beliefs about how hidden states give rise to observations, and how hidden states transition among eachother.\n",
    "\n",
    "In almost all MDPs, the critical building blocks of this generative model are the agent's representation of the observation likelihood, which we'll refer to as `A_gm`, and its representation of the transition likelihood, or `B_gm`. \n",
    "\n",
    "Here, we assume the agent has a veridical representation of the rules of the T-maze (namely, how hidden states cause observations) as well as its ability to control its own movements with certain consequences (i.e. 'noiseless' transitions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the generative model of each agent a copy of the true generative process likelihood array\n",
    "\n",
    "base_A_gm = deepcopy(A_gp) \n",
    "base_B_gm = deepcopy(B_gp) \n",
    "\n",
    "num_obs, num_states, num_modalities, num_factors = utils.get_model_dimensions(A=base_A_gm, B=base_B_gm)\n",
    "\n",
    "base_D_gm = utils.obj_array_uniform(num_states)\n",
    "base_D_gm[0] = utils.onehot(0, num_states[0])\n",
    "\n",
    "base_C_gm = utils.obj_array_zeros(num_obs)\n",
    "base_C_gm[1] = np.array([0., 3., -3.])\n",
    "\n",
    "num_actions = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_agents = 50  # number of different agents \n",
    "\n",
    "# construct all the generative models of all agents by copying the \"base\" generative model\n",
    "# we're putting the batch-dimension (here: `num_agents`) in the leading dimension of each modality- or factor-specific sub-array\n",
    "A_gm_all = [jnp.broadcast_to(jnp.array(a), (num_agents,) + a.shape) for a in base_A_gm]  # map the true observation likelihood to jax arrays\n",
    "B_gm_all = [jnp.broadcast_to(jnp.array(b), (num_agents,) + b.shape) for b in base_B_gm]  # map the true transition likelihood to jax arrays\n",
    "D_gm_all = [jnp.broadcast_to(jnp.array(d),  (num_agents,) + d.shape) for d in base_D_gm]\n",
    "C_gm_all = [jnp.broadcast_to(jnp.array(c),  (num_agents,) + c.shape) for c in base_C_gm]\n",
    "E_gm_all = jnp.ones((num_agents, num_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the `Agent()` class\n",
    "\n",
    "In `pymdp`, we have abstracted much of the computations required for active inference into the `Agent()` class, a flexible object that can be used to store necessary aspects of the generative model, the agent's instantaneous observations and actions, and perform action / perception using functions like `Agent.infer_states` and `Agent.infer_policies`. \n",
    "\n",
    "An instance of `Agent` is straightforwardly initialized with a call to `Agent()` with a list of optional arguments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our call to `Agent()`, we need to constrain the default behavior with some of our T-Maze-specific needs. For example, we want to make sure that the agent's beliefs about transitions are constrained by the fact that it can only control the `Location` factor - _not_ the `Reward Condition` (which we assumed stationary across an epoch of time). Therefore we specify this using a list of indices that will be passed as the `control_fac_idx` argument of the `Agent()` constructor. \n",
    "\n",
    "Each element in the list specifies a hidden state factor (in terms of its index) that is controllable by the agent. Hidden state factors whose indices are _not_ in this list are assumed to be uncontrollable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "controllable_indices = [0] # this is a list of the indices of the hidden state factors that are controllable\n",
    "agent = Agent(A_gm_all, B_gm_all, C_gm_all, D_gm_all, E_gm_all, control_fac_idx=controllable_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTreeDef(CustomNode(Agent[(('A', 'B', 'C', 'D', 'E', 'gamma', 'qs', 'q_pi'), ('num_obs', 'num_modalities', 'num_states', 'num_factors', 'num_controls', 'inference_algo', 'control_fac_idx', 'policy_len', 'policies', 'use_utility', 'use_states_info_gain', 'use_param_info_gain', 'action_selection'), ([4, 3, 2], 3, [4, 2], 2, [4, 1], 'VANILLA', [0], 1, DeviceArray([[[0, 0]],\n",
      "\n",
      "             [[1, 0]],\n",
      "\n",
      "             [[2, 0]],\n",
      "\n",
      "             [[3, 0]]], dtype=int32), True, True, False, 'deterministic'))], [[*, *, *], [*, *], [*, *, *], [*, *], *, *, None, None]))\n"
     ]
    }
   ],
   "source": [
    "import jax.tree_util as jtu\n",
    "\n",
    "vals, tree = jtu.tree_flatten(agent)\n",
    "\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymdp.jax.inference import update_posterior_states as jax_update_posterior\n",
    "from jax.nn import one_hot\n",
    "\n",
    "o_vec = [one_hot(o, base_A_gm[i].shape[0]) for i, o in enumerate(_obs)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_i = 0\n",
    "agent_i_A = [a[agent_i] for a in A_gm_all]\n",
    "agent_i_D = [d[agent_i] for d in D_gm_all]\n",
    "# agent_i_D = [jnp.ones(4)/4., jnp.ones(2)/2.]\n",
    "# o_vec[2] = jnp.array([1.0, 0.0])\n",
    "test_out = jax_update_posterior(\n",
    "            agent_i_A,\n",
    "            o_vec,\n",
    "            prior=agent_i_D\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymdp.jax.maths import compute_log_likelihood,compute_log_likelihood_single_modality, log_stable\n",
    "ll = compute_log_likelihood(o_vec, agent_i_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_log_likelihood_single_modality(o_vec[2], agent_i_A[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = jnp.zeros((4, 2))\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    o_m, A_m = o_vec[i], agent_i_A[i]\n",
    "\n",
    "    expanded_obs = jnp.expand_dims(o_m, tuple(range(1, A_m.ndim)))\n",
    "    likelihood = (expanded_obs * A_m).sum(axis=0, keepdims=True).squeeze()\n",
    "\n",
    "    log_likelihood += log_stable(likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.nn import softmax\n",
    "\n",
    "softmax((log_likelihood * jnp.ones((1,2))/2.0).sum(1) + log_stable(agent_i_D[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax((log_likelihood * jnp.ones((4,1))/4.0).sum(0) + log_stable(agent_i_D[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymdp.inference import update_posterior_states as numpy_update_posterior\n",
    "test_out_np = numpy_update_posterior(base_A_gm, _obs, prior=base_D_gm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Inference\n",
    "Now we can start off the T-maze with an initial observation and run active inference via a loop over a desired time interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "T = 5 # number of timesteps\n",
    "\n",
    "emp_prior = D_gm_all\n",
    "_obs = env.reset() # reset the environment and get an initial observation\n",
    "obs = jnp.broadcast_to(jnp.array(_obs), (num_agents, num_modalities)) # everyone gets the same initial observation\n",
    "\n",
    "agent_to_show = 1 # which agent to print the messages of over time\n",
    "\n",
    "# these are useful for displaying read-outs during the loop over time\n",
    "reward_conditions = [\"Right\", \"Left\"]\n",
    "location_observations = ['CENTER','RIGHT ARM','LEFT ARM','CUE LOCATION']\n",
    "reward_observations = ['No reward','Reward!','Loss!']\n",
    "cue_observations = ['Cue Right','Cue Left']\n",
    "msg = \"\"\" === Starting experiment === \\n Reward condition: {}, Observation: [{}, {}, {}]\"\"\"\n",
    "print(msg.format(reward_conditions[env.reward_condition], location_observations[_obs[0]], reward_observations[_obs[1]], cue_observations[_obs[2]]))\n",
    "\n",
    "qs_list = []\n",
    "measurements = {'actions': [], 'outcomes': [obs]}\n",
    "for t in range(T):\n",
    "    qs = agent.infer_states(obs, emp_prior)\n",
    "    qs_list.append(qs.copy())\n",
    "\n",
    "    q_pi, efe = agent.infer_policies(qs)\n",
    "\n",
    "    actions = agent.sample_action(q_pi)\n",
    "    emp_prior = agent.update_empirical_prior(actions, qs)\n",
    "\n",
    "    measurements[\"actions\"].append( actions )\n",
    "    msg = \"\"\"[Step {}] Action: [Move to {}]\"\"\"\n",
    "    print(msg.format(t, location_observations[int(actions[agent_to_show, 0])]))\n",
    "\n",
    "    obs = []\n",
    "    for a in actions:\n",
    "        obs.append( jnp.array(env.step(list(a))) )\n",
    "    obs = jnp.stack(obs)\n",
    "    measurements[\"outcomes\"].append(obs)\n",
    "\n",
    "    msg = \"\"\"[Step {}] Observation: [{},  {}, {}]\"\"\"\n",
    "    print(msg.format(t, location_observations[obs[agent_to_show, 0]], reward_observations[obs[agent_to_show, 1]], cue_observations[obs[agent_to_show, 2]]))\n",
    "    \n",
    "measurements['actions'] = jnp.stack(measurements['actions']).astype(jnp.int32)\n",
    "measurements['outcomes'] = jnp.stack(measurements['outcomes'])\n",
    "measurements['outcomes'] = measurements['outcomes'][None, :T]\n",
    "measurements['actions'] = measurements['actions'][None]\n",
    "reward_condition_beliefs = jnp.stack([qs_i[1] for qs_i in qs_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(reward_condition_beliefs[0], aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model inversion\n",
    "Define model likelihood given the observed sequence of actions and outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpyro as npyro\n",
    "from jax import random\n",
    "from numpyro.infer import Predictive\n",
    "from pymdp.jax.likelihoods import aif_likelihood, evolve_trials\n",
    "\n",
    "print(measurements['outcomes'].shape)\n",
    "print(measurements['actions'].shape)\n",
    "\n",
    "Nb, Nt, Na, _ = measurements['actions'].shape\n",
    "\n",
    "xs = {'outcomes': measurements['outcomes'][0], 'actions': measurements['actions'][0]}\n",
    "evolve_trials(agent, xs)\n",
    "%timeit evolve_trials(agent, xs)\n",
    "\n",
    "rng_key = random.PRNGKey(0)\n",
    "\n",
    "with npyro.handlers.seed(rng_seed=0):\n",
    "    aif_likelihood(Nb, Nt, Na, measurements, agent)\n",
    "\n",
    "pred_samples = Predictive(aif_likelihood, num_samples=11)(rng_key, Nb, Nt, Na, measurements, agent)\n",
    "%timeit pred_samples = Predictive(aif_likelihood, num_samples=11)(rng_key, Nb, Nt, Na, measurements, agent)\n",
    "print(pred_samples.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpyro as npyro\n",
    "import numpyro.distributions as dist\n",
    "from jax import nn, lax, vmap\n",
    "\n",
    "@vmap\n",
    "def trans_params(z):\n",
    "\n",
    "    a = nn.sigmoid(z[0])\n",
    "    lam = nn.softplus(z[1])\n",
    "    d = nn.sigmoid(z[2])\n",
    "\n",
    "    A = lax.stop_gradient([jnp.array(x) for x in list(base_A_gm)])\n",
    "\n",
    "    middle_matrix1 = jnp.array([[0., 0.], [a, 1-a], [1-a, a]])\n",
    "    middle_matrix2 = jnp.array([[0., 0.], [1-a, a], [a, 1-a]])\n",
    "\n",
    "    side_vector = jnp.stack([jnp.array([1.0, 0., 0.]), jnp.array([1.0, 0., 0.])], -1)\n",
    "\n",
    "    A[1] = jnp.stack([side_vector, middle_matrix1, middle_matrix2, side_vector], -2)\n",
    "    \n",
    "    C = [\n",
    "        jnp.zeros(4),\n",
    "        lam * jnp.array([0., 1., -1.]),\n",
    "        jnp.zeros(2)\n",
    "    ]\n",
    "\n",
    "    D = [nn.one_hot(0, 4), jnp.array([d, 1-d])]\n",
    "\n",
    "    E = jnp.ones(4)/4\n",
    "\n",
    "    params = {\n",
    "        'A': A,\n",
    "        'B': lax.stop_gradient([jnp.array(x) for x in list(base_B_gm)]),\n",
    "        'C': C,\n",
    "        'D': D,\n",
    "        'E': E\n",
    "    }\n",
    "\n",
    "    return  params, a, lam, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, num_blocks, num_steps, num_agents, num_params=3):\n",
    "    with npyro.plate('agents', num_agents):\n",
    "        z = npyro.sample('z', dist.Normal(0., 1.).expand([num_params]).to_event(1))\n",
    "        params, a, lmbd, d = trans_params(z)\n",
    "        # register parameter values\n",
    "        npyro.deterministic('a', a)\n",
    "        npyro.deterministic('lambda', lmbd)\n",
    "        npyro.deterministic('d', d)\n",
    "\n",
    "    agents = Agent(\n",
    "        params['A'], \n",
    "        params['B'], \n",
    "        params['C'], \n",
    "        params['D'], \n",
    "        params['E'], \n",
    "        control_fac_idx=controllable_indices\n",
    "    )\n",
    "\n",
    "    aif_likelihood(num_blocks, num_steps, num_agents, data, agents)\n",
    "    \n",
    "with npyro.handlers.seed(rng_seed=101111):\n",
    "    model(measurements, Nb, Nt, Na)\n",
    "\n",
    "%timeit pred_samples = Predictive(model, num_samples=11)(rng_key, measurements, Nb, Nt, Na)\n",
    "pred_samples = Predictive(model, num_samples=11)(rng_key, measurements, Nb, Nt, Na)\n",
    "print(pred_samples.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference with NUTS and MCMC\n",
    "from numpyro.infer import NUTS, MCMC\n",
    "from numpyro.infer import init_to_feasible, init_to_sample\n",
    "\n",
    "rng_key = random.PRNGKey(0)\n",
    "kernel = NUTS(model, init_strategy=init_to_feasible)\n",
    "\n",
    "mcmc = MCMC(kernel, num_warmup=1000, num_samples=1000, progress_bar=False)\n",
    "\n",
    "rng_key, _rng_key = random.split(rng_key)\n",
    "mcmc.run(_rng_key, measurements, Nb, Nt, Na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "az.style.use('arviz-darkgrid')\n",
    "\n",
    "coords = {\n",
    "    'idx': jnp.arange(num_agents),\n",
    "    'vars': jnp.arange(3), \n",
    "}\n",
    "dims = {'z': [\"idx\", \"vars\"], 'd': [\"idx\"], 'lambda': [\"idx\"], 'a': [\"idx\"]}\n",
    "data_kwargs = {\n",
    "    \"dims\": dims,\n",
    "    \"coords\": coords,\n",
    "}\n",
    "data_mcmc = az.from_numpyro(posterior=mcmc, **data_kwargs)\n",
    "az.plot_trace(data_mcmc, kind=\"rank_bars\", var_names=['d', 'lambda', 'a']);\n",
    "\n",
    "#TODO: maybe plot real values on top of samples from the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference with SVI and autoguides\n",
    "import optax\n",
    "from numpyro.infer import SVI, Trace_ELBO, Predictive\n",
    "from numpyro.infer.autoguide import AutoMultivariateNormal\n",
    "\n",
    "num_iters = 1000\n",
    "guide = AutoMultivariateNormal(model)\n",
    "optimizer = npyro.optim.optax_to_numpyro(optax.chain(optax.adabelief(1e-3)))\n",
    "svi = SVI(model, guide, optimizer, Trace_ELBO(num_particles=10))\n",
    "rng_key, _rng_key = random.split(rng_key)\n",
    "svi_res = svi.run(_rng_key, num_iters, measurements, Nb, Nt, Na, progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "plt.plot(svi_res.losses)\n",
    "plt.ylabel('Variational free energy');\n",
    "plt.xlabel('iter step');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key, _rng_key = random.split(rng_key)\n",
    "pred = Predictive(\n",
    "    model, \n",
    "    guide=guide, \n",
    "    params=svi_res.params, \n",
    "    num_samples=1000, \n",
    "    return_sites=[\"d\", \"a\", \"lambda\"]\n",
    ")\n",
    "post_sample = pred(_rng_key, measurements, Nb, Nt, Na)\n",
    "\n",
    "for key in post_sample:\n",
    "    post_sample[key] = jnp.expand_dims(post_sample[key], 0)\n",
    "\n",
    "data_svi = az.convert_to_inference_data(post_sample, group=\"posterior\", **data_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = az.plot_forest(\n",
    "    [data_mcmc, data_svi],\n",
    "    model_names = [\"nuts\", \"svi\"],\n",
    "    kind='forestplot',\n",
    "    var_names=['d', 'lambda', 'a'],\n",
    "    coords={\"idx\": 0},\n",
    "    combined=True,\n",
    "    figsize=(20, 6)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pymdp_env3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "32c08a4ac355ebac62cad37715f1d18a3925a14af2b6a4a96942ab426da83c5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
