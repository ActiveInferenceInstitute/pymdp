{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Inference Demo: T-Maze Environment\n",
    "This demo notebook provides a full walk-through of active inference using the `Agent()` class of `inferactively`. The canonical example used here is the 'T-maze' task, often used in the active inference literature [cite stuff here]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "First, import `inferactively` and the modules we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import numpy as np\n",
    "\n",
    "path = pathlib.Path(os.getcwd())\n",
    "module_path = str(path.parent.parent) + '/'\n",
    "sys.path.append(module_path)\n",
    "\n",
    "from inferactively.agent import Agent\n",
    "from inferactively import core\n",
    "from inferactively.distributions import Categorical, Dirichlet\n",
    "from inferactively.envs import TMazeEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Functions\n",
    "\n",
    "Define some demo-specific auxiliary functions that will be helpful for plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "Here we consider an agent navigating a three-armed 'T-maze,' with the agent starting in a central location of the maze. The bottom arm of the maze contains an informative cue, which signals in which of the two top arms ('Left' or 'Right', the ends of the 'T') a reward is likely to be found. \n",
    "\n",
    "At each timestep, the environment is described by the joint occurrence of two qualitatively-different 'kinds' of states (hereafter referred to as _hidden state factors_). These hidden state factors are independent of one another.\n",
    "\n",
    "We represent the first hidden state factor (`Location`) as a $ 1 \\ x \\ 4 $ vector that encodes the current position of the agent, and can take the following values: {`CENTER`, `RIGHT ARM`, `LEFT ARM`, or `CUE LOCATION`}. For example, if the agent is in the `CUE LOCATION`, the current state of this factor would be $s_1 = [0 \\ 0 \\ 0 \\ 1]$.\n",
    "\n",
    "We represent the second hidden state factor (`Reward Condition`) as a $ 1 \\ x \\ 2 $ vector that encodes the reward condition of the trial: {`Reward on Right`, or `Reward on Left`}.  A trial where the condition is reward is `Reward on Left` is thus encoded as the state $s_2 = [0 \\ 1]$.\n",
    "\n",
    "The environment is designed such that when the agent is located in the `RIGHT ARM` and the reward condition is `Reward on Right`, the agent has a specified probability $a$ (where $a > 0.5$) of receiving a reward, and a low probability $b = 1 - a$ of receiving a 'loss' (we can think of this as an aversive or unpreferred stimulus). If the agent is in the `LEFT ARM` for the same reward condition, the reward probabilities are swapped, and the agent experiences loss with probability $a$, and reward with lower probability $b = 1 - a$. These reward contingencies are intuitively swapped for the `Reward on Left` condition. \n",
    "\n",
    "For instance, we can encode the state of the environment at the first time step in a `Reward on Right` trial with the following pair of hidden state vectors: $s_1 = [1 \\ 0 \\ 0 \\ 0]$, $s_2 = [1 \\ 0]$, where we assume the agent starts sitting in the central location. If the agent moved to the right arm, then the corresponding hidden state vectors would now be $s_1 = [0 \\ 1 \\ 0 \\ 0]$, $s_2 = [1 \\ 0]$. This highlights the _independence_ of the two hidden state factors -- the location of the agent ($s_1$) can change without affecting the identity of the reward condition ($s_2$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initialize environment\n",
    "Now we can initialize the T-maze environment using the built-in `TMazeEnv` class from the `inferactively.envs` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose reward probabilities $a$ and $b$, where $a$ and $b$ are the probabilities of reward / loss in the 'correct' arm, and the probabilities of loss / reward in the 'incorrect' arm. Which arm counts as 'correct' vs. 'incorrect' depends on the reward condition (state of the 2nd hidden state factor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_probabilities = [0.98, 0.02] # probabilities used in Karl's original SPM demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize an instance of the T-maze environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TMazeEnv(reward_probs = reward_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure of the state --> outcome mapping\n",
    "We can 'peer into' the rules encoded by the environment (also known as the _generative process_) by looking at the probability distributions that map from hidden states to observations. Following the SPM version of active inference, we refer to this collection of probabilistic relationships as the $A$ array. In the case of the true rules of the environment, we refer to this array as `A_gp` (where `gp` denotes the generative process). \n",
    "\n",
    "It is worth outlining what the observations are in this task. Here, we have three sensory channels or observation modalities: `Location`, `Reward`, and `Cue`. \n",
    "\n",
    ">The `Location` observation values are identical to the `Location` hidden state values. In this case, the agent always unambiguously observes its own state - if the agent is in `RIGHT ARM`, it receives a `RIGHT ARM` observation in the corresponding modality. This might be analogized to a 'proprioceptive' sense of place.\n",
    "\n",
    ">The `Reward` observation modality assumes the values `No Reward`, `Reward` or `Loss`. The `No Reward` (index 0) observation is  observed whenever the agent isn't occupying one of the two T-maze arms (the right or left arms). The `Reward` (index 1) and `Loss` (index 2) observations are observed in the right and left arms of the T-maze, with associated probabilities that depend on the reward condition (i.e. on the value of the second hidden state factor).\n",
    "\n",
    "> The `Cue` observation modality assumes the values `Cue Right`, `Cue Left`. This observation unambiguously signals the reward condition of the trial, and therefore in which arm the `Reward` observation is more probable. When the agent occupies the other arms, the `Cue` observation will be `Cue Right` or `Cue Left` with equal probability. However (as we'll see below when we intialise the agent), the agent's beliefs about the likelihood mapping render these observations uninformative and irrelevant to state inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_gp = env.get_likelihood_dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Categorical Distribution> \n",
       " [[1.   0.   0.   1.  ]\n",
       " [0.   0.98 0.02 0.  ]\n",
       " [0.   0.02 0.98 0.  ]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_gp[1][:,:,0] # mapping between arms and reward probabilities in 'Reward Right' condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Categorical Distribution> \n",
       " [[1.   0.   0.   1.  ]\n",
       " [0.   0.02 0.98 0.  ]\n",
       " [0.   0.98 0.02 0.  ]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_gp[1][:,:,1] # mapping between arms and reward probabilities in 'Reward Left' condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_gp = env.get_transition_dist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The generative model\n",
    "Now we can move onto setting up the generative model of the agent - namely, the agent's beliefs about how hidden states give rise to observations and how its own actions affect hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_gm = A_gp.copy()\n",
    "B_gm = B_gp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(A=A_gm, B=B_gm, control_fac_idx=[0])\n",
    "agent.C[1][1] = 3.0\n",
    "agent.C[1][2] = -3.0\n",
    "T = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " === Starting experiment === \n",
      " Reward condition: Reward on Left, Initial observation (0, 0, 1) \n",
      "[0] Inference [Arm 0 / reward 0] \n",
      "[Step 0] Action: [Move to Arm 1]\n",
      "[Step 0] Observation: [Arm 1, Reward 1]\n",
      "[1] Inference [Arm 1 / reward 0] \n",
      "[Step 1] Action: [Move to Arm 1]\n",
      "[Step 1] Observation: [Arm 1, Reward 1]\n",
      "[2] Inference [Arm 1 / reward 0] \n",
      "[Step 2] Action: [Move to Arm 1]\n",
      "[Step 2] Observation: [Arm 1, Reward 1]\n",
      "[3] Inference [Arm 1 / reward 0] \n",
      "[Step 3] Action: [Move to Arm 1]\n",
      "[Step 3] Observation: [Arm 1, Reward 1]\n",
      "[4] Inference [Arm 1 / reward 0] \n",
      "[Step 4] Action: [Move to Arm 1]\n",
      "[Step 4] Observation: [Arm 1, Reward 1]\n",
      "[5] Inference [Arm 1 / reward 0] \n",
      "[Step 5] Action: [Move to Arm 1]\n",
      "[Step 5] Observation: [Arm 1, Reward 1]\n",
      "[6] Inference [Arm 1 / reward 0] \n",
      "[Step 6] Action: [Move to Arm 1]\n",
      "[Step 6] Observation: [Arm 1, Reward 1]\n",
      "[7] Inference [Arm 1 / reward 0] \n",
      "[Step 7] Action: [Move to Arm 1]\n",
      "[Step 7] Observation: [Arm 1, Reward 1]\n",
      "[8] Inference [Arm 1 / reward 0] \n",
      "[Step 8] Action: [Move to Arm 1]\n",
      "[Step 8] Observation: [Arm 1, Reward 2]\n",
      "[9] Inference [Arm 1 / reward 0] \n",
      "[Step 9] Action: [Move to Arm 1]\n",
      "[Step 9] Observation: [Arm 1, Reward 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/conor/infer-actively/inferactively/distributions/categorical.py:311: UserWarning: You have called :log: on a Categorical that contains zeros.                      We have removed zeros by adding a small non-negative scalar to each value.\n",
      "  We have removed zeros by adding a small non-negative scalar to each value.\"\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "reward_conditions = [\"Reward on Left\", \"Reward on Right\"]\n",
    "msg = \"\"\" === Starting experiment === \\n Reward condition: {}, Initial observation {} \"\"\"\n",
    "print(msg.format(reward_conditions[env.reward_condition], obs))\n",
    "\n",
    "for t in range(T):\n",
    "    qx = agent.infer_states(obs)\n",
    "    msg = \"\"\"[{}] Inference [Arm {} / reward {}] \"\"\"\n",
    "    print(msg.format(t, qx[0].sample(), qx[1].sample(), obs[0], obs[1]))\n",
    "\n",
    "    q_pi, efe = agent.infer_policies()\n",
    "\n",
    "    action = agent.sample_action()\n",
    "\n",
    "    msg = \"\"\"[Step {}] Action: [Move to Arm {}]\"\"\"\n",
    "    print(msg.format(t, action[0]))\n",
    "\n",
    "    obs = env.step(action)\n",
    "\n",
    "    msg = \"\"\"[Step {}] Observation: [Arm {}, Reward {}]\"\"\"\n",
    "    print(msg.format(t, obs[0], obs[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
