{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Inference: Multi-Factor Generative Model\n",
    "This notebook simulates an active inference agent behaving in a random environment described by two hidden state variables and a single observation modality. The agent uses variational inference to infer the most likely hidden states, and optimizes its policies with respect to those that minimize the expected free energy of their attendant observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import basic paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "path = Path(os.getcwd())\n",
    "module_path = str(path.parent) + '/'\n",
    "sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import `inferactively` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import special\n",
    "\n",
    "from inferactively.distributions import Categorical, Dirichlet\n",
    "from inferactively import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an auxiliary function for creating the transition likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_B(Ns, Nf, controllableActionIdx):\n",
    "    \"\"\"\n",
    "    Generate controlled transitions for each hidden state factor, that correspond to actions.\n",
    "    \"\"\"\n",
    "\n",
    "    B = np.empty((Nf),dtype=object)\n",
    "    for si, ndim_si in enumerate(Ns):\n",
    "        B[si] = np.eye(ndim_si)\n",
    "\n",
    "    # controllable hidden state factors - transition to the k-th location\n",
    "\n",
    "    for pi in controllableActionIdx:\n",
    "        B[pi] = np.tile(B[pi].reshape(Ns[pi],Ns[pi],1),(1,1,Ns[pi])).transpose((1,2,0))\n",
    "    \n",
    "    return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The generative process\n",
    "Here, we setup the mechanics of the environment, or the 'generative process.' To make this analogous to the generative _model_ learned by the agent, we describe these mechanics using likelihood distribution $P(o_t|s_t)$, denoted `A_GP`, and a transition distribution $P(s_t|s_{t-1},a_{t-1})$, denoted `B_GP`. The generative process will be used to generate observations `obs` via the likelihood $P(o_t|s_t)$ and is changed by actions via the likelihood $P(s_t|s_{t-1},a_{t-1})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up state-space and outcome-space dimensionalities of the generative process\n",
    "No = [4]     # dimensionality of the different outcome modalities\n",
    "Ng = len(No) # total number of outcome modalities\n",
    "\n",
    "Ns = [3, 2]  # dimensionality of the hidden state factors\n",
    "Nf = len(Ns) # total number of hidden state factors\n",
    "\n",
    "# Create the likelihoods and priors relevant to the generative model\n",
    "if Ng == 1:\n",
    "    A_GP = Categorical(values = np.random.rand(*(No+Ns)))\n",
    "    A_GP.normalize()\n",
    "else:\n",
    "    A_GP = np.empty(Ng, dtype = object)\n",
    "    for g in range(Ng):\n",
    "        A_GP[g] = np.random.rand(*(No[g] + Ns))\n",
    "    A_GP = Categorical(values = A_GP)\n",
    "    A_GP.normalize()\n",
    "\n",
    "B_GP = Categorical(values = create_B(Ns, Nf, [0, 1]) )\n",
    "\n",
    "initState = np.array( [np.eye(Ns[f])[np.random.randint(Ns[f])] for f in range(Nf) ], dtype = object)\n",
    "\n",
    "T = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The generative model\n",
    "Here, we setup the belief structure of the active inference agent, or the 'generative model.' For this simple case, we make the generative model identical to the generative process. Namely, the agent's beliefs about the observation and likelihood distributions (respectively, the _observation model_ and _transition model_ ) are identical to the true parameters describing the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative model likelihoods\n",
    "A_GM = Categorical(values = A_GP.values) # in this case, the generative model and the generative process are identical\n",
    "B_GM = Categorical(values = B_GP.values) # in this case, the generative model and the generative process are identical\n",
    "\n",
    "# Prior Dirichlet parameters (these parameterize the generative model likelihoods)\n",
    "pA = Dirichlet(values = A_GM.values * 1e20) # fix prior beliefs about observation likelihood to be really high (and thus impervious to learning)\n",
    "pB = Dirichlet(values = B_GP.values * 1e20) # fix prior beliefs about transition likelihood to be really high (and thus impervious to learning)\n",
    "\n",
    "# create some preference over observations\n",
    "\n",
    "if Ng == 1:\n",
    "    C = np.zeros(*No)\n",
    "    C[0] = -2 # prefers not to observe the outcome with index == 0\n",
    "    C[-1] = 2 # prefers to observe the outcome with highest index\n",
    "else:\n",
    "    C = np.empty(Ng, dtype = object)\n",
    "    for g in range(Ng):\n",
    "        C[g] = np.zeros(No[g])\n",
    "    C[0][0] = -2\n",
    "    C[0][-1] = 2\n",
    "\n",
    "prior = Categorical(values = np.array([np.ones(Ns[f])/Ns[f] for f in range(Nf)], dtype = object))\n",
    "\n",
    "# policy related parameters\n",
    "policy_horizon = 1\n",
    "cntrl_fac_idx = [0, 1]\n",
    "Nu, possiblePolicies = F.constructNu(Ns,Nf,cntrl_fac_idx,policy_horizon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action-Perception Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize history of beliefs, hidden states, and observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Run the action perception loop\n",
    "s = initState\n",
    "\n",
    "# set up some variables to store history of actions, etc.\n",
    "actions_hist = np.zeros( (len(Nu),T) )\n",
    "states_hist = np.zeros( (Nf,T) )\n",
    "obs_hist = np.zeros( (Ng,T) )\n",
    "Qs_hist = np.empty(Nf, dtype=object)\n",
    "for f in range(Nf):\n",
    "    Qs_hist[f] = np.zeros( (Ns[f], T) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(T):\n",
    "\n",
    "    #### STEP ONE: SAMPLE AN OBSERVATION FROM THE GENERATIVE PROCESS ######\n",
    "    ps = A_GP.dot(s)\n",
    "    obs = ps.sample()\n",
    "\n",
    "    #### STEP TWO: INVERT GENERATIVE MODEL TO INFER MOST LIKELY HIDDEN STATE ######\n",
    "\n",
    "    Qs = F.update_posterior_states(A_GM, obs, prior, return_numpy = False)\n",
    "\n",
    "    #### STEP THREE: INFER THE MOST LIKELY POLICIES (USING EXPECTED FREE ENERGY ASSUMPTION) #####\n",
    "\n",
    "    Q_pi,EFE = F.update_posterior_policies(Qs, A_GM, pA, B_GM, pB, C, possiblePolicies, gamma = 16.0, return_numpy=True)\n",
    "\n",
    "    #### STEP FOUR: SAMPLE AN ACTION FROM THE POSTERIOR OVER CONTROLS, AND PERTURB THE GENERATIVE PROCESS USING THE SAMPLED ACTION #####\n",
    "    action = F.sample_action(Q_pi, possiblePolicies, Nu, sampling_type = 'marginal_action')\n",
    "\n",
    "    s = np.array( [B_GP[f][:,:,a_i].dot(s[f],return_numpy=True) for f, a_i in enumerate(action)], dtype=object) \n",
    "    \n",
    "     #### STORE VARIABLES IN HISTORY ####\n",
    "    actions_hist[:,t] = np.array(action)\n",
    "    for f in range(Nf):\n",
    "        states_hist[f,t] = np.where(s[f])[0]\n",
    "\n",
    "    obs_hist[:,t] = np.array(obs)\n",
    "    \n",
    "    for f in range(Nf):\n",
    "        Qs_hist[f][:,t] = Qs[f].values[:,0].copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
